# This tutorial aims to teach you how to set up the debezium with sql server.

# PRE-REQS
You must import the AMQ Streams Kafka Image, please always check the right version https://catalog.redhat.com/software/containers/amq7/amq-streams-kafka-33-rhel8/637e0ea240d971f5448e6a0f[AMQ Streams 3.3.1]
```shell
oc import-image amq7/amq-streams-kafka-33-rhel8:2.4.0-5 --from=registry.redhat.io/amq7/amq-streams-kafka-33-rhel8:2.4.0-5 --confirm
```
```shell
oc tag amq-streams-kafka-33-rhel8:2.4.0-5 debezium-streams-connect:1.0
```
If you cluster doesn't have access, you can use a traditional way, using the pull and push, let's take a look in an example:
```shell
$ oc get nodes
$ oc debug nodes/<node_name>
sh-4.4# chroot /host
sh-4.4# oc login -u <username> --server=<API_URL>:6443
sh-4.4# podman login -u kubeadmin -p $(oc whoami -t) image-registry.openshift-image-registry.svc:5000
sh-4.4# podman pull 
sh-4.4# podman login registry.redhat.io 
sh-4.4# podman pull amq7/amq-streams-kafka-33-rhel8:2.4.0-5
sh-4.4# podman tag amq7/amq-streams-kafka-31-rhel8 image-registry.openshift-image-registry.svc:5000/amq-streams/connect-cluster 
sh-4.4# podman tag amq7/amq-streams-kafka-31-rhel8 image-registry.openshift-image-registry.svc:5000/amq-streams/connect-cluster
sh-4.4# podman push --remove-signatures image-registry.openshift-image-registry.svc:5000/amq-streams/connect-cluster
```

If you are going to use the admin user, you need to create it

```yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaUser
metadata:
  name: admin
  labels:
    strimzi.io/cluster: kafka-cluster
  namespace: amq-streams
spec:
  authentication:
    type: scram-sha-512
```


Let's create a topic that will be use by debezium.
```yaml
kind: KafkaTopic
metadata:
  labels:
    strimzi.io/cluster: kafka-cluster
  name: application-audit
  namespace: amq-streams
spec:
  partitions: 3
  replicas: 3
  config:
    min.insync.replicas: 2
    retention.ms: 604800000
    segment.bytes: 1073741824
```


Now we need to create the topics that will be used to store and manager the Kafka Workers from the KafkaConnect. You can change the name if you want then you just need to use the new name in kafkaconnect properties:

* offset.storage.topic: my-connect-cluster-offsets (1)
* config.storage.topic: my-connect-cluster-configs (2)
* status.storage.topic: my-connect-cluster-status  (3)

1. Kafka topic that stores connector offsets.
2. Kafka topic that stores connector and task status configurations.
3. Kafka topic that stores connector and task status updates.

```yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: connect-cluster-configs
  labels:
    strimzi.io/cluster: kafka-cluster
  namespace: amq-streams
spec:
  config:
    cleanup.policy: compact
  partitions: 1
  replicas: 3
  topicName: connect-cluster-configs
---
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: connect-cluster-offsets
  labels:
    strimzi.io/cluster: kafka-cluster
  namespace: amq-streams
spec:
  config:
    cleanup.policy: compact
  partitions: 25
  replicas: 3
  topicName: connect-cluster-offsets

---
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: connect-cluster-status
  labels:
    strimzi.io/cluster: kafka-cluster
  namespace: amq-streams
spec:
  config:
    cleanup.policy: compact
  partitions: 5
  replicas: 3
  topicName: connect-cluster-status
```

Optional, You can create a new user for the kafkaconnect or use the admin, you if wanna proceed with a new user if you must give the right permission.
By default, the SQL Server connector writes events for all INSERT, UPDATE, and DELETE operations that occur in a table to a single Apache Kafka topic that is specific to that table. The connector uses the following convention to name change event topics: <topicPrefix>.<schemaName>.<tableName>
```yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaUser
metadata:
  name: debezium-mssql
  labels:
    strimzi.io/cluster: kafka-cluster
  namespace: amq-streams
spec:
  authentication:
    type: scram-sha-512
  authorization:
    acls:
      - host: '*'
        operations:
          - Write
          - Create
          - Read
          - Write
        resource:
          name: connect-cluster-offsets
          patternType: literal
          type: topic
      - host: '*'
        operations:
          - Write
          - Create
          - Read
          - Write
        resource:
          name: connect-cluster-status
          patternType: literal
          type: topic
      - host: '*'
        operations:
          - Write
          - Create
          - Read
          - Write
        resource:
          name: connect-cluster-configs
          patternType: literal
          type: topic
      - host: '*'
        operations:
          - Read
        resource:
          name: connect-cluster
          patternType: literal
          type: group

    type: simple
```

Creating the KafkaConnect
```yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnect
metadata:
  annotations:
    strimzi.io/use-connector-resources: 'true'
  name: debezium-connect-sqlserver
  namespace: amq-streams
spec:
  authentication:
    passwordSecret:
      password: password
      secretName: user-debezium-mssql
    type: scram-sha-512
    username: debezium-mssql
  bootstrapServers: 'kafka-cluster-kafka-bootstrap:9093'
  build:
    output:
      image: 'debezium-streams-connect:1.0'
      type: imagestream
    plugins:
      - artifacts:
          - type: zip
            url: >-
              https://maven.repository.redhat.com/ga/io/debezium/debezium-connector-sqlserver/2.1.4.Final-redhat-00001/debezium-connector-sqlserver-2.1.4.Final-redhat-00001-plugin.zip
        name: debezium-connector-sqlserver
  config:
    config.storage.replication.factor: -1
    config.storage.topic: connect-cluster-configs
    group.id: connect-cluster
    offset.storage.replication.factor: -1
    offset.storage.topic: connect-cluster-offsets
    status.storage.replication.factor: -1
    status.storage.topic: connect-cluster-status
  replicas: 1
  tls:
    trustedCertificates:
      - certificate: ca.crt
        secretName: kafka-cluster-cluster-ca-cert
  version: 3.3.1
```
Creating topic that will be used by cdc, in my case the name of the table is tblAuthors.
```yaml
---
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  labels:
    strimzi.io/cluster: kafka
  name: sqlserver
spec:
  config:
    min.insync.replicas: '3'
    retention.ms: '604800000'
    segment.bytes: '1073741824'
  partitions: 3
  replicas: 3
  topicName: sqlserver
---
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  labels:
    strimzi.io/cluster: kafka
  name: sqlserver-tblauthors
spec:
  config:
    min.insync.replicas: '3'
    retention.ms: '604800000'
    segment.bytes: '1073741824'
  partitions: 3
  replicas: 3
  topicName: sqlserver.dtb_enterprise.dbo.tblAuthors
```
