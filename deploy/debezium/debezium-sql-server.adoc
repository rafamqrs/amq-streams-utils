# This tutorial aims to teach you how to set up the debezium with sql server.

# PRE-REQS
You must import the AMQ Streams Kafka Image, please always check the right version https://catalog.redhat.com/software/containers/amq7/amq-streams-kafka-33-rhel8/637e0ea240d971f5448e6a0f[AMQ Streams 3.3.1]
```shell
oc import-image amq7/amq-streams-kafka-33-rhel8:2.4.0-5 --from=registry.redhat.io/amq7/amq-streams-kafka-33-rhel8:2.4.0-5 --confirm
```
```shell
oc tag amq-streams-kafka-33-rhel8:2.4.0-5 debezium-streams-connect:1.0
```
If you cluster doesn't have access, you can use a traditional way, using the pull and push, let's take a look in an example:
```shell
$ oc get nodes
$ oc debug nodes/<node_name>
sh-4.4# chroot /host
sh-4.4# oc login -u <username> --server=<API_URL>:6443
sh-4.4# podman login -u kubeadmin -p $(oc whoami -t) image-registry.openshift-image-registry.svc:5000
sh-4.4# podman pull 
sh-4.4# podman login registry.redhat.io 
sh-4.4# podman pull amq7/amq-streams-kafka-33-rhel8:2.4.0-5
sh-4.4# podman tag amq7/amq-streams-kafka-31-rhel8 image-registry.openshift-image-registry.svc:5000/amq-streams/connect-cluster 
sh-4.4# podman tag amq7/amq-streams-kafka-31-rhel8 image-registry.openshift-image-registry.svc:5000/amq-streams/connect-cluster
sh-4.4# podman push --remove-signatures image-registry.openshift-image-registry.svc:5000/amq-streams/connect-cluster
```

If you are going to use the admin user, you need to create it

```yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaUser
metadata:
  name: admin
  labels:
    strimzi.io/cluster: kafka-cluster
  namespace: amq-streams
spec:
  authentication:
    type: scram-sha-512
```


Let's create a topic that will be use by debezium.
```yaml
kind: KafkaTopic
metadata:
  labels:
    strimzi.io/cluster: kafka-cluster
  name: application-audit
  namespace: amq-streams
spec:
  partitions: 3
  replicas: 3
  config:
    min.insync.replicas: 2
    retention.ms: 604800000
    segment.bytes: 1073741824
```

(OPTIONAL)
Now we need to create the topics that will be used to store and manager the Kafka Workers from the KafkaConnect. You can change the name if you want then you just need to use the new name in kafkaconnect properties:

* offset.storage.topic: my-connect-cluster-offsets (1)
* config.storage.topic: my-connect-cluster-configs (2)
* status.storage.topic: my-connect-cluster-status  (3)

1. Kafka topic that stores connector offsets.
2. Kafka topic that stores connector and task status configurations.
3. Kafka topic that stores connector and task status updates.

```yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: connect-cluster-configs
  labels:
    strimzi.io/cluster: kafka-cluster
  namespace: amq-streams
spec:
  config:
    cleanup.policy: compact
  partitions: 1
  replicas: 3
  topicName: connect-cluster-configs
---
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: connect-cluster-offsets
  labels:
    strimzi.io/cluster: kafka-cluster
  namespace: amq-streams
spec:
  config:
    cleanup.policy: compact
  partitions: 25
  replicas: 3
  topicName: connect-cluster-offsets

---
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: connect-cluster-status
  labels:
    strimzi.io/cluster: kafka-cluster
  namespace: amq-streams
spec:
  config:
    cleanup.policy: compact
  partitions: 5
  replicas: 3
  topicName: connect-cluster-status
```

Optional, You can create a new user for the kafkaconnect or use the admin, you if wanna proceed with a new user if you must give the right permission.
By default, the SQL Server connector writes events for all INSERT, UPDATE, and DELETE operations that occur in a table to a single Apache Kafka topic that is specific to that table. The connector uses the following convention to name change event topics: <topicPrefix>.<schemaName>.<tableName>
```yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaUser
metadata:
  name: debezium-mssql
  labels:
    strimzi.io/cluster: kafka-cluster
  namespace: amq-streams
spec:
  authentication:
    type: scram-sha-512
  authorization:
    acls:
      - host: '*'
        operations:
          - Write
          - Create
          - Read
          - Write
        resource:
          name: connect-cluster-offsets
          patternType: literal
          type: topic
      - host: '*'
        operations:
          - Write
          - Create
          - Read
          - Write
        resource:
          name: connect-cluster-status
          patternType: literal
          type: topic
      - host: '*'
        operations:
          - Write
          - Create
          - Read
          - Write
        resource:
          name: connect-cluster-configs
          patternType: literal
          type: topic
      - host: '*'
        operations:
          - Read
        resource:
          name: connect-cluster
          patternType: literal
          type: group

    type: simple
```

Creating the KafkaConnect
```yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnect
metadata:
  annotations:
    strimzi.io/use-connector-resources: 'true'
  name: debezium-connect-sqlserver
  namespace: amq-streams
spec:
  authentication:
    passwordSecret:
      password: password
      secretName: user-debezium-mssql
    type: scram-sha-512
    username: debezium-mssql
  bootstrapServers: 'kafka-cluster-kafka-bootstrap:9093'
  build:
    output:
      image: 'debezium-streams-connect:1.0'
      type: imagestream
    plugins:
      - artifacts:
          - type: zip
            url: >-
              https://maven.repository.redhat.com/ga/io/debezium/debezium-connector-sqlserver/2.1.4.Final-redhat-00001/debezium-connector-sqlserver-2.1.4.Final-redhat-00001-plugin.zip
        name: debezium-connector-sqlserver
  config:
    config.storage.replication.factor: -1
    config.storage.topic: connect-cluster-configs
    group.id: connect-cluster
    offset.storage.replication.factor: -1
    offset.storage.topic: connect-cluster-offsets
    status.storage.replication.factor: -1
    status.storage.topic: connect-cluster-status
  replicas: 1
  tls:
    trustedCertificates:
      - certificate: ca.crt
        secretName: kafka-cluster-cluster-ca-cert
  version: 3.3.1
```

Before we create the KafkaConnector, let's prepare the Database.

1 - Create the Login and User
```sql
--CREATE LOGIN debeziumLogin WITH PASSWORD = 'Sql2019isfast';
--CREATE USER debezium FOR LOGIN debeziumLogin;
```

2 - Grant to the user the select on tblAuthors
```sql
--USE dtb_enterprise
--GO
--Grant select on tblAuthors to debezium
```

3 - Create Table tblAuthors 
```sql
--CREATE Table tblAuthors
--(
--   Id int identity primary key,
--   Author_name nvarchar(50),
--   country nvarchar(50)
--)
```

4 - (Optional) you can insert some random datas
```sql
--Declare @Id int
--Set @Id = 1
--
--While @Id <= 120000
--Begin 
--   Insert Into tblAuthors values ('Author - ' + CAST(@Id as nvarchar(10)),
--              'Country - ' + CAST(@Id as nvarchar(10)) + ' name')
--   Print @Id
--   Set @Id = @Id + 1
--End
```

5 - Enable CDC on SQL Server
```sql
--EXEC sys.sp_cdc_enable_db

--Enabling CDC on the SQL Server database(https://debezium.io/documentation/reference/stable/connectors/sqlserver.html#_enabling_cdc_on_the_sql_server_database)
--EXEC dtb_enterprise..sp_addsrvrolemember @loginame = N'debeziumLogin', @rolename = N'sysadmin'

--Enabling CDC on a SQL Server table(https://debezium.io/documentation/reference/stable/connectors/sqlserver.html#_enabling_cdc_on_a_sql_server_table)
--EXEC sys.sp_cdc_enable_table
--@source_schema = N'dbo',
--@source_name = N'tblAuthors', 
--@role_name = N'NULL',  
--@filegroup_name = N'PRIMARY',
--@supports_net_changes = 0

--Verifying that the user has access to the CDC table (https://debezium.io/documentation/reference/stable/connectors/sqlserver.html#_verifying_that_the_user_has_access_to_the_cdc_table)
--EXEC sys.sp_cdc_help_change_data_capture
```

Creating topic that will be used by cdc, in my case the name of the table is tblAuthors, The connector uses the following convention to name change event topics: <topicPrefix>.<schemaName>.<tableName>.
```yaml
---
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  labels:
    strimzi.io/cluster: kafka
  name: sqlserver
spec:
  config:
    min.insync.replicas: '3'
    retention.ms: '604800000'
    segment.bytes: '1073741824'
  partitions: 3
  replicas: 3
  topicName: sqlserver
---
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  labels:
    strimzi.io/cluster: kafka
  name: sqlserver-tblauthors
spec:
  config:
    min.insync.replicas: '3'
    retention.ms: '604800000'
    segment.bytes: '1073741824'
  partitions: 3
  replicas: 3
  topicName: sqlserver.dtb_enterprise.dbo.tblAuthors
```

Finally, we can create the KafkaConnector.
```yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnector
metadata:
  labels:
    strimzi.io/cluster: debezium-connect-sqlserver
  name: enterprise-connector-sqlserver
  namespace: amq-streams
spec:
  class: io.debezium.connector.sqlserver.SqlServerConnector
  config:
    schema.history.internal.consumer.sasl.mechanism: SCRAM-SHA-512
    table.include.list: dbo.tblAuthors
    schema.history.internal.producer.sasl.mechanism: SCRAM-SHA-512
    schema.history.internal.producer.ssl.truststore.location: /opt/kafka/connect-certs/kafka-cluster-cluster-ca-cert/ca.p12
    database.hostname: <REPLACE>
    schema.history.internal.consumer.ssl.type: PKCS12
    schema.history.internal.consumer.ssl.truststore.location: /opt/kafka/connect-certs/kafka-cluster-cluster-ca-cert/ca.p12
    schema.history.internal.producer.ssl.truststore.password: <REPLACE>
    snapshot.mode: initial
    schema.history.internal.producer.ssl.type: PKCS12
    database.password: <REPLACE>
    schema.history.internal.consumer.sasl.jaas.config: >-
      org.apache.kafka.common.security.scram.ScramLoginModule required
      username="admin" password="<REPLACE>";
    schema.history.internal.consumer.ssl.truststore.password: <REPLACE>
    topic.prefix: sqlserver
    database.encrypt: false
    schema.history.internal.producer.sasl.jaas.config: >-
      org.apache.kafka.common.security.scram.ScramLoginModule required
      username="admin" password="<REPLACE>";
    database.port: 31433
    schema.history.internal.producer.security.protocol: SASL_SSL
    database.names: dtb_enterprise
    schema.history.internal.consumer.security.protocol: SASL_SSL
    schema.history.internal.kafka.topic: schemahistory.dtb_enterprise
    database.user: debeziumLogin
    schema.history.internal.kafka.bootstrap.servers: 'kafka-cluster-kafka-bootstrap:9093'
  tasksMax: 1
```
